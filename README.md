# Ali Vosoughi  â­ [![Total Stars](https://img.shields.io/github/stars/ali-vosoughi?affiliations=COLLABORATOR&style=social)](https://github.com/ali-vosoughi) â­

ğŸ“ PhD researcher at [Prof. Chenliang Xu's Lab](http://www.cs.rochester.edu/~cxu22/) | Unified multimodal reasoning, understanding, and generation | [alivosoughi.com](https://alivosoughi.com)

---

ğŸ‘‹ Welcome to Ali Vosoughi's code repository. I try to provide value for you, so I put some of my work here so you can better search through the codes and find what you need.

ğŸ§  My interest is in systems beyond human capabilitiesâ€”not just through intelligence, but through combining visual, auditory, and other signals [EEG, behavior, haptics, lidar, IR, ultrasound] that evolution forgot to put in us. Look at bats, for exampleâ€”they see signals we can't see! ğŸ¦‡

ğŸ”¬ Currently we're limited to audio, visual, and semantic modalities. Today we connect video and images with language models, and the importance of language has made multimodal branches extremely popular. Consider work on [3D perspective understanding](https://github.com/yunlong10/MMPerspective) ğŸ—ï¸, [video understanding with LLMs](https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding) ğŸ¥, [visual question answering](https://github.com/ali-vosoughi/PW-VQA) â“, and [visual reasoning](https://huggingface.co/datasets/jing-bi/verify-teaser) ğŸ§©. But these works cannot fill the audio gap at all. 

ğŸµ For example, you can refer to several examples of multimodal audio work: [SoundCLIP](https://github.com/ali-vosoughi/SoundCLIP) ğŸ”Š, [counterfactual audio learning](https://github.com/ali-vosoughi/counterfactual-audio) ğŸ¯, and [multimodal instructional system with augmented reality](https://github.com/ali-vosoughi/misar) ğŸ¼. Audio reasoning with semantic domain support is also possible, or audio and video can help each other with source separation, and the semantic domain can play a role in connections between vision, language, and audio.

ğŸš€ These systems can ultimately understand, comprehend, and generate unified outputs between vision, language, speech, audio, and videoâ€”though not like natural humans, but to get us to our goals faster.
